#! /usr/bin/python3
# -*- coding: utf-8 -*-
"""markov wordwise with my own sparse matrix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oERqeYb5Emy1dIrUNgpg6Zci1xq2a2SP
"""

import numpy as np
import requests
import re
import sys
import time

from tqdm import tqdm
from sparsematrix import SparseMatrix
from enumeratingtokenizer import EnumeratingTokenizer
from textbrushing import clean_html, text_brushing




try:
    file_name = sys.argv[1]

    f = open(file_name, "r")
    text = f.read(500*10**6)


except:

    file_name = "warandpeace"
    url = "http://az.lib.ru/t/twen_m/text_1884_the_adventures_of_huckleberry_finn.shtml"
    # url = "http://www.lib.ru/LINDGREN/malysh.txt_Ascii.txt"
    text = requests.get(url).text



tokens_are_words = 0

try:
    markov_dim = int(sys.argv[2])
except:
    markov_dim = 3

print(f"Next word predicted based on {markov_dim} previous words")

if tokens_are_words:
    text = clean_html(text)
    text = text.split()
else:
    text = text.replace("\n", "")


tknz = EnumeratingTokenizer(text)
text_digitized = (
    tknz.tokenize_text_by_enumerating()
)  # Converting text into a list of integers
unique_symbols = tknz.unique_symbols


def calculate_transition_matrix(text_digitized, markov_dim, len_uniques=None):
    """ markov_dim - Number of tokens to take into account while predicting the next token. The transition
        matrix dimension is always markov_dim + 1
    """
    if not len_uniques:
        len_uniques = max(text_digitized)

    prob_matrix = SparseMatrix([len_uniques] * (markov_dim + 1))
    for i in tqdm(range(markov_dim, len(text_digitized))):
        n_gram_and_char = tuple(text_digitized[i - markov_dim : i + 1])
        prob_matrix[n_gram_and_char] += 1

    return prob_matrix


prob_matrix = calculate_transition_matrix(text_digitized, markov_dim)


if tokens_are_words:
    n_tokens_to_generate = 300
else:
    n_tokens_to_generate = 1500
chars_in_line = 100

a = np.random.randint(0, len(text_digitized) - markov_dim)
left = tuple(text_digitized[a : markov_dim + a])
print("Начинаем генерирование текста: ", left)

start = time.time()

for i in range(n_tokens_to_generate):
    distr = np.array(
        [prob_matrix[(left + (x,))] for x in range(0, prob_matrix.shape[0])]
    )

    if not sum(distr):
        continue

    distr_offset = 0 # Bigger values smoothen probability peaks
    #print(sorted(distr)[-10:])
    #print(np.std(distr))
    distr += distr_offset
    distr[distr <= distr_offset] = 0



    distr = distr / sum(distr)
    char = np.random.choice(range(prob_matrix.shape[0]), p=distr)
    print(unique_symbols[char], end=" " if tokens_are_words else "", flush=True)
    if i % chars_in_line == 0:
        print()
    left = (left + (char,))[1: ]

print(f"\nTime spent: {round(time.time() - start, 4)} sec")